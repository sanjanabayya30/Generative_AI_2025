{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1hjLX_9r7rdDUBIJJLQXG3gpEsuCC8rdD",
      "authorship_tag": "ABX9TyPC+NDa/xTiPgy8yt6DontT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanjanabayya30/Generative_AI_2025/blob/main/GENAI_ProjectCode_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries (Colab-specific installation)\n",
        "!pip install -q gradio huggingface_hub\n",
        "\n",
        "# Import necessary libraries\n",
        "import gradio as gr\n",
        "from huggingface_hub import InferenceClient, login\n",
        "import getpass\n",
        "\n",
        "# Authenticate with Hugging Face\n",
        "HF_TOKEN = getpass.getpass(\"Enter your Hugging Face token: \")\n",
        "login(token=HF_TOKEN)\n",
        "\n",
        "# Initialize Hugging Face Inference Client\n",
        "client = InferenceClient(\"HuggingFaceH4/zephyr-7b-beta\")\n",
        "\n",
        "# Define function to generate story (same as original)\n",
        "def generate_story(messages, max_tokens, temperature, top_p):\n",
        "    formatted_prompt = \"\"\n",
        "    for msg in messages:\n",
        "        role = msg[\"role\"]\n",
        "        content = msg[\"content\"]\n",
        "        if role == \"system\":\n",
        "            formatted_prompt += f\"<|system|>\\n{content}</s>\\n\"\n",
        "        elif role == \"user\":\n",
        "            formatted_prompt += f\"<|user|>\\nWrite a story about: {content}</s>\\n\"\n",
        "        elif role == \"assistant\":\n",
        "            formatted_prompt += f\"<|assistant|>\\n{content}</s>\\n\"\n",
        "    formatted_prompt += \"<|assistant|>\\n\"\n",
        "    return client.text_generation(\n",
        "        formatted_prompt,\n",
        "        max_new_tokens=max_tokens,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "    )\n",
        "\n",
        "# Define response function (same as original)\n",
        "def respond(message, chat_history, max_tokens, temperature, top_p):\n",
        "    system_message = {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are an AI story generator. Always respond with creative stories based on user inputs.\",\n",
        "    }\n",
        "    messages = [system_message] + chat_history + [{\"role\": \"user\", \"content\": message}]\n",
        "    story = generate_story(messages, max_tokens, temperature, top_p)\n",
        "    updated_history = chat_history + [\n",
        "        {\"role\": \"user\", \"content\": message},\n",
        "        {\"role\": \"assistant\", \"content\": story},\n",
        "    ]\n",
        "    return updated_history, \"\"\n",
        "\n",
        "# Create and launch Gradio interface (Colab-optimized)\n",
        "def run_colab_interface():\n",
        "    demo = gr.Blocks()\n",
        "\n",
        "    with demo:\n",
        "        gr.Markdown(\"# 📖 AI Story Generator (Colab Version)\")\n",
        "        with gr.Row():\n",
        "            chatbot = gr.Chatbot(\n",
        "                type=\"messages\",\n",
        "                label=\"Story Session\",\n",
        "                height=500,\n",
        "                container=False\n",
        "            )\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=4):\n",
        "                user_input = gr.Textbox(\n",
        "                    label=\"Your Story Prompt\",\n",
        "                    placeholder=\"Enter your story idea...\",\n",
        "                    container=False\n",
        "                )\n",
        "                submit_btn = gr.Button(\"Generate\", variant=\"primary\")\n",
        "\n",
        "            with gr.Column(scale=1):\n",
        "                max_tokens = gr.Slider(128, 2048, 512, step=64, label=\"Story Length\")\n",
        "                temperature = gr.Slider(0.1, 1.5, 0.7, step=0.1, label=\"Creativity\")\n",
        "                top_p = gr.Slider(0.1, 1.0, 0.95, step=0.05, label=\"Focus\")\n",
        "                clear_btn = gr.Button(\"Clear History\", variant=\"secondary\")\n",
        "\n",
        "        # Event handlers\n",
        "        submit_btn.click(\n",
        "            respond,\n",
        "            [user_input, chatbot, max_tokens, temperature, top_p],\n",
        "            [chatbot, user_input]\n",
        "        )\n",
        "        user_input.submit(\n",
        "            respond,\n",
        "            [user_input, chatbot, max_tokens, temperature, top_p],\n",
        "            [chatbot, user_input]\n",
        "        )\n",
        "        clear_btn.click(\n",
        "            lambda: [], None, chatbot, queue=False\n",
        "        )\n",
        "\n",
        "    # Colab-specific launch configuration\n",
        "    demo.launch(debug=True, share=True, height=800)\n",
        "\n",
        "# Run the interface\n",
        "if __name__ == \"__main__\":\n",
        "    run_colab_interface()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 929
        },
        "id": "9NiDsUKt7cIj",
        "outputId": "07dbcbcf-5610-432e-88dc-b876b31b1743"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your Hugging Face token: ··········\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://0e2e2792195a19a6fe.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://0e2e2792195a19a6fe.gradio.live\" width=\"100%\" height=\"800\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}